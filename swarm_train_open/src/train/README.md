## PPO Training

In this folder implements PPO training with modified [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) 0.6.3.

We implement customized sampling and fixed a few bugs via modifying [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) code. Note that these modifications are only tested in our environment and may not work in others.

This provides a demo to show how to apply our framework in RL training.